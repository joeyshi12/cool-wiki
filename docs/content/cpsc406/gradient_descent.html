<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Gradient Descent</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Gradient Descent</h1>
<p class="subtitle"><a href="/">Return home</a></p>
</header>
<table>
<colgroup>
<col style="width: 33%" />
<col style="width: 32%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Method</th>
<th>Problem</th>
<th><span class="math inline">\(d_k\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Gauss-Newton</td>
<td><span class="math inline">\(\min_{x}
\frac{1}{2}\|r(x)\|^2\)</span></td>
<td><span class="math inline">\(-(J_k^TJ_k)^{-1}(J_k^Tr_k)\)</span></td>
</tr>
<tr class="even">
<td>Pure Newton</td>
<td>General unconstrained</td>
<td><span class="math inline">\(-H_k^{-1}g_k\)</span></td>
</tr>
<tr class="odd">
<td>Scaled Descent</td>
<td>General unconstrained</td>
<td><span class="math inline">\(-SS^Tg_k\)</span></td>
</tr>
<tr class="even">
<td>Reduced Gradient Method</td>
<td>Linearly constrained</td>
<td><span class="math inline">\(-(Z^TH_kZ)^{-1}Z^Tg_k\)</span></td>
</tr>
</tbody>
</table>
<h2 id="descent-algorithm">Descent Algorithm</h2>
<ul>
<li>choose initial <span class="math inline">\(x_0\)</span></li>
<li>for <span class="math inline">\(k = 0,1,2,\dots\)</span>
<ul>
<li>compute <span class="math inline">\(d_k\)</span></li>
<li>linesearch on <span class="math inline">\(f(x_k + \alpha
d_k)\)</span></li>
<li><span class="math inline">\(x_{k+1} = x_k + d_k\)</span></li>
<li>if <span class="math inline">\(\|\nabla f(x_k)\| &lt;
\epsilon\)</span>, then return</li>
</ul></li>
</ul>
<h2 id="linesearch">Linesearch</h2>
<ul>
<li>choose initial <span class="math inline">\(t\leftarrow
t_0\)</span></li>
<li>for <span class="math inline">\(k = 0,1,2,\dots\)</span>
<ul>
<li>if <span class="math inline">\(f(x_k + td) - f_k \leq \alpha t
\nabla f_k^T d\)</span>, then return</li>
<li><span class="math inline">\(t\leftarrow \beta t\)</span></li>
</ul></li>
</ul>
<figure>
<img src="/assets/linesearch.svg" alt="Linesearch" />
<figcaption aria-hidden="true">Linesearch</figcaption>
</figure>
<h2 id="convergence">Convergence</h2>
<p><strong>definition.</strong> <span class="math inline">\(f\)</span>
is <span class="math inline">\(L\)</span>-<strong>Lipschitz continuously
differentiable</strong> if</p>
<p><span class="math display">\[\|\nabla f(x) - \nabla f(y)\leq L\|x -
y\|,\quad \forall x, y\in\mathbb{R}^n.\]</span></p>
<ul>
<li><span class="math inline">\(f(x) = \frac{1}{2}x^TAx + b^Tx
\Rightarrow L = \|A\|_2 = \lambda_{\max}(A)\)</span></li>
</ul>
<p><strong>definition.</strong> <span class="math inline">\(f\)</span>
is <span class="math inline">\(\mu\)</span>-<strong>strongly
convex</strong> (<span class="math inline">\(\mu &gt; 0\)</span>) if</p>
<p><span class="math display">\[f(z) \geq f(x) + \nabla f(x)^T(z - x) +
\frac{\mu}{2}\|z - x\|^2,\quad \forall x,z\in\mathbb{R}^n.\]</span></p>
<p>If <span class="math inline">\(f\)</span> is twice continuously
differentiable, this is equivalent to</p>
<p><span class="math display">\[\lambda_{\min}(\nabla^2 f(x))\geq
\mu,\quad \forall x\in\mathbb{R}^n.\]</span></p>
<p><strong>proposition.</strong> Let <span
class="math inline">\(f\)</span> be <span
class="math inline">\(L\)</span>-Lipschitz continuously differentiable.
Let <span class="math inline">\(\{x_k\}\)</span> be the iterates of
gradient descent with constant stepsize <span
class="math inline">\(\alpha\in (0, 2/L)\)</span>. Then, <span
class="math inline">\(\min_{k=0,\dots, T} \|\nabla f(x_k)\|^2 \leq =
\frac{2(f_0 - f^*)}{\alpha T} = O(1/T)\)</span>.</p>
<p>Moreover, if <span class="math inline">\(f\)</span> is <span
class="math inline">\(\mu\)</span>-strongly convex, then the number of
iterates is at most <span class="math inline">\(\frac{L}{\mu}\log\left(
\frac{f(x_0) - f^*}{\epsilon} \right)\)</span>, where <span
class="math inline">\(\epsilon :=\)</span> tolerance.</p>
<h2 id="stochastic-gradient-descent">Stochastic Gradient Descent</h2>
<p>Let <span class="math inline">\(f(x) = \frac{1}{N}(f_{1}(x) + \cdots
+ f_{N}(x))\)</span>. In stochastic gradient descent, at each iteration
<span class="math inline">\(k\)</span>, we sample a batch of indices
<span class="math inline">\(i\in B_k\)</span>, <span
class="math inline">\(i\sim \mbox{Unif}\{1,\dots, N\}\)</span> and
approximate the gradient with <span class="math inline">\(g_k =
\frac{1}{|B_k|}\sum_{i\in B_k} \nabla f_i(x_k)\)</span>.</p>
<p>Compared to regular gradient descent, stochastic gradient descent
will require more iterations, but can be much more computationally
cheaper.</p>
</body>
</html>
